{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3V9P8uHX_nuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openmim\n",
        "!mim install mmcv-full\n",
        "!git clone https://github.com/open-mmlab/mmsegmentation.git\n",
        "%cd mmsegmentation\n",
        "!pip install -v -e .\n",
        "# \"-v\" means verbose, or more output\n",
        "# \"-e\" means installing a project in editable mode,\n",
        "# thus any local modifications made to the code will take effect without reinstallation.\n",
        "%cd ../"
      ],
      "metadata": {
        "id": "Ev2rJ8n7tn3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown 1EyaZVdbezIJsj8LviM7GaIBto46a1N-Z\n",
        "!gdown 1L8NYh3LOSGf7xNm7TsZVXURbYYfeJVKh\n",
        "!gdown 1m8fsG812o6KotF1NVo0YuiSfSn18TAOA\n",
        "!gdown 1d3wU8KNjPL4EqMCIEO_rO-O3-REpG82T\n",
        "!gdown 1BUtU42moYrOFbsMCE-LTTkUE-mrWnfG2\n",
        "!gdown 1d7I50jVjtCddnhpf-lqj8-f13UyCzoW1\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b0.pth mit_b0_mmseg.pth\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b1.pth mit_b1_mmseg.pth\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b2.pth mit_b2_mmseg.pth\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b3.pth mit_b3_mmseg.pth\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b4.pth mit_b4_mmseg.pth\n",
        "!python mmsegmentation/tools/model_converters/mit2mmseg.py mit_b5.pth mit_b5_mmseg.pth"
      ],
      "metadata": {
        "id": "1SMf2ybTtqXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFlkmPvPN_gl"
      },
      "outputs": [],
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tqdm\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations.augmentations import transforms\n",
        "from albumentations.core.composition import Compose, OneOf\n",
        "from albumentations.augmentations.transforms import RandomGamma\n",
        "import os"
      ],
      "metadata": {
        "id": "AQePAlbAXq67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainsize = 384\n",
        "\n",
        "class BKpolypDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dir=\"path/to/data\", transform=None):\n",
        "        self.img_path_lst = []\n",
        "        self.dir = dir\n",
        "        self.transform = transform\n",
        "        self.img_path_lst = glob.glob(\"{}/train/train/*\".format(self.dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path_lst)\n",
        "\n",
        "    def read_mask(self, mask_path):\n",
        "        image = cv2.imread(mask_path)\n",
        "        image = cv2.resize(image, (trainsize, trainsize))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        # lower boundary RED color range values; Hue (0 - 10)\n",
        "        lower1 = np.array([0, 100, 20])\n",
        "        upper1 = np.array([10, 255, 255])\n",
        "        # upper boundary RED color range values; Hue (160 - 180)\n",
        "        lower2 = np.array([160,100,20])\n",
        "        upper2 = np.array([179,255,255])\n",
        "        lower_mask = cv2.inRange(image, lower1, upper1)\n",
        "        upper_mask = cv2.inRange(image, lower2, upper2)\n",
        "        red_mask = lower_mask + upper_mask;\n",
        "        red_mask[red_mask != 0] = 2\n",
        "        # boundary RED color range values; Hue (36 - 70)\n",
        "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255,255))\n",
        "        green_mask[green_mask != 0] = 1\n",
        "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
        "        full_mask = full_mask.astype(np.uint8)\n",
        "        return full_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_path_lst[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (trainsize, trainsize))\n",
        "        label_path = img_path.replace(\"train\", \"train_gt\")\n",
        "        label = self.read_mask(label_path)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=label)\n",
        "            image = transformed[\"image\"]\n",
        "            label = transformed[\"mask\"]\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "9YpB8T-mXjl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
        "    A.RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
        "    A.OneOf([A.Blur(), A.GaussianBlur(), A.GlassBlur(), A.MotionBlur(), A.GaussNoise(), A.Sharpen(), A.MedianBlur(), A.MultiplicativeNoise()]),\n",
        "    A.Cutout(p=0.2, max_h_size=35, max_w_size=35, fill_value=255),\n",
        "    A.RandomSnow(snow_point_lower=0.1, snow_point_upper=0.15, brightness_coeff=1.5, p=0.09),\n",
        "    A.RandomShadow(p=0.1),\n",
        "    A.ShiftScaleRotate(p=0.45, border_mode=cv2.BORDER_CONSTANT, shift_limit=0.15, scale_limit=0.15),\n",
        "    A.RandomCrop(trainsize, trainsize),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406),std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "ll2YO9_YYHw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BKpolypDataset(dir=\"/content\", transform=train_transform)"
      ],
      "metadata": {
        "id": "LC_FKjFpYmkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnNormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "        Returns:\n",
        "            Tensor: Normalized image.\n",
        "        \"\"\"\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "    \n",
        "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
      ],
      "metadata": {
        "id": "PcLAkcOFYnpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "id_test = random.randint(0, train_dataset.__len__())\n",
        "sample1, sample2 = train_dataset.__getitem__(id_test)\n",
        "print(torch.unique(sample2))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(unorm(sample1).permute(1, 2, 0))\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(sample2.squeeze())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PdQHxvAkYdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "FW1qSpjgkZw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metrics\n",
        "#tham khao: https://github.com/hszhao/semseg/blob/master/util/util.py\n",
        "def intersectionAndUnionGPU(output, target, K, ignore_index=255):\n",
        "    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n",
        "    assert (output.dim() in [1, 2, 3])\n",
        "    assert output.shape == target.shape\n",
        "    output = output.view(-1)\n",
        "    target = target.view(-1)\n",
        "    output[target == ignore_index] = ignore_index\n",
        "    intersection = output[output == target]\n",
        "    area_intersection = torch.histc(intersection, bins=K, min=0, max=K-1)\n",
        "    area_output = torch.histc(output, bins=K, min=0, max=K-1)\n",
        "    area_target = torch.histc(target, bins=K, min=0, max=K-1)\n",
        "    area_union = area_output + area_target - area_intersection\n",
        "    return area_intersection, area_union, area_target "
      ],
      "metadata": {
        "id": "3YtZ1CpEpIYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss_Ori(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
        "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
        "    Focal_Loss= -1*alpha*((1-pt)**gamma)*log(pt)\n",
        "    Args:\n",
        "        num_class: number of classes\n",
        "        alpha: class balance factor\n",
        "        gamma:\n",
        "        ignore_index:\n",
        "        reduction:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_class, alpha=None, gamma=2, ignore_index=None, reduction='mean'):\n",
        "        super(FocalLoss_Ori, self).__init__()\n",
        "        self.num_class = num_class\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.smooth = 1e-4\n",
        "        self.ignore_index = ignore_index\n",
        "        self.alpha = alpha\n",
        "        if alpha is None:\n",
        "            self.alpha = torch.ones(num_class, )\n",
        "        elif isinstance(alpha, (int, float)):\n",
        "            self.alpha = torch.as_tensor([alpha] * num_class)\n",
        "        elif isinstance(alpha, (list, np.ndarray)):\n",
        "            self.alpha = torch.as_tensor(alpha)\n",
        "        if self.alpha.shape[0] != num_class:\n",
        "            raise RuntimeError('the length not equal to number of class')\n",
        "\n",
        "        # if isinstance(self.alpha, (list, tuple, np.ndarray)):\n",
        "        #     assert len(self.alpha) == self.num_class\n",
        "        #     self.alpha = torch.Tensor(list(self.alpha))\n",
        "        # elif isinstance(self.alpha, (float, int)):\n",
        "        #     assert 0 < self.alpha < 1.0, 'alpha should be in `(0,1)`)'\n",
        "        #     assert balance_index > -1\n",
        "        #     alpha = torch.ones((self.num_class))\n",
        "        #     alpha *= 1 - self.alpha\n",
        "        #     alpha[balance_index] = self.alpha\n",
        "        #     self.alpha = alpha\n",
        "        # elif isinstance(self.alpha, torch.Tensor):\n",
        "        #     self.alpha = self.alpha\n",
        "        # else:\n",
        "        #     raise TypeError('Not support alpha type, expect `int|float|list|tuple|torch.Tensor`')\n",
        "\n",
        "    def forward(self, logit, target):\n",
        "        # assert isinstance(self.alpha,torch.Tensor)\\\n",
        "        N, C = logit.shape[:2]\n",
        "        alpha = self.alpha.to(logit.device)\n",
        "        prob = F.softmax(logit, dim=1)\n",
        "        if prob.dim() > 2:\n",
        "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
        "            prob = prob.view(N, C, -1)\n",
        "            prob = prob.transpose(1, 2).contiguous()  # [N,C,d1*d2..] -> [N,d1*d2..,C]\n",
        "            prob = prob.view(-1, prob.size(-1))  # [N,d1*d2..,C]-> [N*d1*d2..,C]\n",
        "        ori_shp = target.shape\n",
        "        target = target.view(-1, 1)  # [N,d1,d2,...]->[N*d1*d2*...,1]\n",
        "        valid_mask = None\n",
        "        if self.ignore_index is not None:\n",
        "            valid_mask = target != self.ignore_index\n",
        "            target = target * valid_mask\n",
        "\n",
        "        # ----------memory saving way--------\n",
        "        prob = prob.gather(1, target).view(-1) + self.smooth  # avoid nan\n",
        "        logpt = torch.log(prob)\n",
        "        # alpha_class = alpha.gather(0, target.view(-1))\n",
        "        alpha_class = alpha[target.squeeze().long()]\n",
        "        class_weight = -alpha_class * torch.pow(torch.sub(1.0, prob), self.gamma)\n",
        "        loss = class_weight * logpt\n",
        "        if valid_mask is not None:\n",
        "            loss = loss * valid_mask.squeeze()\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "            if valid_mask is not None:\n",
        "                loss = loss.sum() / valid_mask.sum()\n",
        "        elif self.reduction == 'none':\n",
        "            loss = loss.view(ori_shp)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "fbRQ684zvNMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#model settings\n",
        "norm_cfg = dict(type='BN', requires_grad=True)\n",
        "model_cfg = dict(\n",
        "    type='EncoderDecoder',\n",
        "    pretrained=None,\n",
        "    backbone=dict(\n",
        "        type='MixVisionTransformer',\n",
        "        in_channels=3,\n",
        "        embed_dims=64,\n",
        "        num_stages=4,\n",
        "        num_layers=[3, 8, 27, 3],\n",
        "        num_heads=[1, 2, 5, 8],\n",
        "        patch_sizes=[7, 3, 3, 3],\n",
        "        sr_ratios=[8, 4, 2, 1],\n",
        "        out_indices=(0, 1, 2, 3),\n",
        "        mlp_ratio=4,\n",
        "        qkv_bias=True,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.1,\n",
        "        init_cfg = dict(type=\"Pretrained\", checkpoint=\"mit_b4_mmseg.pth\")),\n",
        "    decode_head=dict(\n",
        "        type='SegformerHead',\n",
        "        in_channels=[64, 128, 320, 512],\n",
        "        in_index=[0, 1, 2, 3],\n",
        "        channels=256,\n",
        "        dropout_ratio=0.1,\n",
        "        num_classes=3,\n",
        "        norm_cfg=norm_cfg,\n",
        "        align_corners=False,\n",
        "        loss_decode=dict(\n",
        "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n",
        "    # model training and testing settings\n",
        "    train_cfg=dict(),\n",
        "    test_cfg=dict(mode='whole'))\n",
        "\n",
        "\n",
        "#load data\n",
        "batch_size = 12\n",
        "n_workers = os.cpu_count()\n",
        "print(\"num_workers =\", n_workers)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=n_workers)\n",
        "\n",
        "import mmcv\n",
        "from mmseg.models import build_segmentor\n",
        "model = build_segmentor(model_cfg).to(device)\n",
        "model.init_weights()\n",
        "\n",
        "#loss\n",
        "criterion = FocalLoss_Ori(num_class=3)\n",
        "\n",
        "#optimizer\n",
        "n_eps = 50\n",
        "init_lr = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
        "                                    T_max=len(train_loader)*n_eps,\n",
        "                                    eta_min=init_lr/100)\n",
        "total_step = len(train_loader)\n",
        "\n",
        "#meter\n",
        "train_loss_meter = AverageMeter()\n",
        "intersection_meter = AverageMeter()\n",
        "union_meter = AverageMeter()\n",
        "target_meter = AverageMeter()"
      ],
      "metadata": {
        "id": "KyBQfeS9pXni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(1, 1+n_eps):\n",
        "    train_loss_meter.reset()\n",
        "    intersection_meter.reset()\n",
        "    union_meter.reset()\n",
        "    target_meter.reset()\n",
        "    model.train()\n",
        "\n",
        "    for batch_id, (x, y) in enumerate(tqdm.tqdm(train_loader), start=1):\n",
        "        if ep <= 1:\n",
        "            optimizer.param_groups[0][\"lr\"] = (ep * batch_id) / (1.0 * total_step) * init_lr\n",
        "        else:\n",
        "            scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        n = x.shape[0]\n",
        "        x = x.to(device).float()\n",
        "        y = y.to(device).long()\n",
        "        y_hat = model.forward_dummy(x) #(B, C, H, W)\n",
        "        loss = criterion(y_hat, y) #(B, C, H, W) >< (B, H, W)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #save metrics\n",
        "        with torch.no_grad():\n",
        "            train_loss_meter.update(loss.item())\n",
        "            y_hat_mask = y_hat.argmax(dim=1).squeeze(1) # (B, C, H, W) -> (B, 1, H, W) -> (B, H, W)\n",
        "            intersection, union, target = intersectionAndUnionGPU(y_hat_mask.float(), y.float(), 3)\n",
        "            intersection_meter.update(intersection)\n",
        "            union_meter.update(union)\n",
        "            target_meter.update(target)\n",
        "\n",
        "    #compute iou, dice\n",
        "    with torch.no_grad():\n",
        "        iou_class = intersection_meter.sum / (union_meter.sum + 1e-10) #vector 3D\n",
        "        dice_class = (2 * intersection_meter.sum) / (intersection_meter.sum + union_meter.sum + 1e-10) #vector 3D\n",
        "\n",
        "        mIoU = torch.mean(iou_class[1:]) #mean iou class 1 and class 2\n",
        "        mDice = torch.mean(dice_class[1:]) #mean dice class 1 and class 2\n",
        "\n",
        "    print(\"EP {}, current_lr = {} , train loss = {} IoU = {}, dice = {}\".format(ep, scheduler.get_last_lr(), train_loss_meter.avg, mIoU, mDice))\n",
        "\n",
        "    if ep >= 40:\n",
        "        torch.save(model.state_dict(), \"modelSegFormer_ep_{}.pth\".format(ep))"
      ],
      "metadata": {
        "id": "TyVvQtbpv1WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "color_dict = {0: (0,   0, 0),\n",
        "              1: (0, 255,   0),\n",
        "              2: (0, 0,   255)}"
      ],
      "metadata": {
        "id": "REv75reBwOiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_to_rgb(mask, color_dict):\n",
        "    output = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    for k in color_dict.keys():\n",
        "        output[mask==k] = color_dict[k]\n",
        "    return np.uint8(output)"
      ],
      "metadata": {
        "id": "UXcmd_AmwVkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir test_mask\n",
        "!mkdir test_overlapmask"
      ],
      "metadata": {
        "id": "6SIkocJlwZoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "for i in os.listdir(\"test/test\"):\n",
        "    img_path = os.path.join(\"test/test\", i)\n",
        "    ori_img = cv2.imread(img_path)\n",
        "    ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
        "    ori_w = ori_img.shape[0]\n",
        "    ori_h = ori_img.shape[1]\n",
        "    img = cv2.resize(ori_img, (trainsize, trainsize))\n",
        "    transformed = val_transform(image=img)\n",
        "    input_img = transformed[\"image\"]\n",
        "    input_img = input_img.unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "      output_mask = model.forward_dummy(input_img).squeeze(0).cpu().numpy().transpose(1,2,0)\n",
        "    mask = cv2.resize(output_mask, (ori_h, ori_w))\n",
        "    mask = np.argmax(mask, axis=-1)\n",
        "    new_rgb_mask = np.zeros((*mask.shape, 3)).astype(np.uint8)\n",
        "    mask_rgb = mask_to_rgb(mask, color_dict)\n",
        "    mask_rgb_true = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n",
        "    overlap = 0.7*ori_img+0.3*mask_rgb_true\n",
        "    overlap = overlap.astype('uint8')\n",
        "    overlap = cv2.cvtColor(overlap, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite(\"test_mask/{}\".format(i), mask_rgb)\n",
        "    cv2.imwrite(\"test_overlapmask/{}\".format(i), overlap)\n",
        "    print(\"processed \", img_path)"
      ],
      "metadata": {
        "id": "N7X4erIewemH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def rle_to_string(runs):\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def rle_encode_one_mask(mask):\n",
        "    pixels = mask.flatten()\n",
        "    pixels[pixels > 225] = 255\n",
        "    pixels[pixels <= 225] = 0\n",
        "    use_padding = False\n",
        "    if pixels[0] or pixels[-1]:\n",
        "        use_padding = True\n",
        "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
        "        pixel_padded[1:-1] = pixels\n",
        "        pixels = pixel_padded\n",
        "    \n",
        "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
        "    if use_padding:\n",
        "        rle = rle - 1\n",
        "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
        "    return rle_to_string(rle)\n",
        "\n",
        "def rle2mask(mask_rle, shape=(3,3)):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (width,height) of array to return \n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape).T\n",
        "\n",
        "def mask2string(dir):\n",
        "    ## mask --> string\n",
        "    strings = []\n",
        "    ids = []\n",
        "    ws, hs = [[] for i in range(2)]\n",
        "    for image_id in os.listdir(dir):\n",
        "        id = image_id.split('.')[0]\n",
        "        path = os.path.join(dir, image_id)\n",
        "        print(path)\n",
        "        img = cv2.imread(path)[:,:,::-1]\n",
        "        h, w = img.shape[0], img.shape[1]\n",
        "        for channel in range(2):\n",
        "            ws.append(w)\n",
        "            hs.append(h)\n",
        "            ids.append(f'{id}_{channel}')\n",
        "            string = rle_encode_one_mask(img[:,:,channel])\n",
        "            strings.append(string)\n",
        "    r = {\n",
        "        'ids': ids,\n",
        "        'strings': strings,\n",
        "    }\n",
        "    return r\n",
        "\n",
        "\n",
        "MASK_DIR_PATH = 'test_mask' # change this to the path to your output mask folder\n",
        "dir = MASK_DIR_PATH\n",
        "res = mask2string(dir)\n",
        "df = pd.DataFrame(columns=['Id', 'Expected'])\n",
        "df['Id'] = res['ids']\n",
        "df['Expected'] = res['strings']\n",
        "\n",
        "df.to_csv(r'output.csv', index=False)"
      ],
      "metadata": {
        "id": "z5Fb-Kmuw1dU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}